% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/var_select (do not include in release).R
\name{var_select_interactions_2way}
\alias{var_select_interactions_2way}
\title{Parallel natural evolutionary variable selection for 2 way latent score interactions (for IMLEGIT)}
\usage{
var_select_interactions_2way(x, y, data, x_2 = NULL, parallel_iter = 3,
  alpha = c(1, 5, 10), eps_nes = 0.001, popsize = 25, lr = 0.2,
  prop_ignored = 0.5, search_criterion = "AICc", n_cluster = 1,
  eps = 0.01, maxiter = 100, family = gaussian, ylim = NULL,
  seed = NULL, progress = TRUE, cv_iter = 5, cv_folds = 5,
  folds = NULL, Huber_p = 1.345, classification = FALSE)
}
\arguments{
\item{x}{string with the names of datasets containing different contructs for potential latent scores. Ex: Assuming 3 datasets: Depression_prenatal, Depression_postnatal, Genes; we run the following LEGIT models : Depression_prenatal x Depression_postnatal, Depression_prenatal x Genes, Depression_postnatal x Genes and we find the best subsets of variable for all of them.}

\item{y}{vector of strings with the names of the outcomes, must be inside "data". We fit the models for each outcome individually.}

\item{data}{data.frame of the dataset to be used.}

\item{x_2}{Optional string with the names of datasets containing different contructs for B. If this is used, then we assume "x" contains the names for A.}

\item{parallel_iter}{number of parallel tries (Default = 3). For speed, I recommend using the number of CPU cores.}

\item{alpha}{vector of the parameter for the Dirichlet distribution of the starting points (Assuming a symmetric Dirichlet distribution with only one parameter). If the vector has size N and parralel_iter=K, we use alpha[1], ..., alpha[N], alpha[1], ... , alpha[N], ... for parallel_iter 1 to K respectively. We assume a dirichlet distribution for the starting points to get a bit more variability and make sure we are not missing on a great subset of variable that doesn't converge to the global optimum with the default starting points. Use bigger values for less variability and lower values for more variability (Default = c(1,5,10)).}

\item{eps_nes}{Threshold for convergence, recommended not to change (Default = .001).}

\item{popsize}{Size of the population, the number of subsets of variables sampled at each iteration (Default = 25). Between 25 and 100 is generally adequate.}

\item{lr}{learning rate of the gradient descent, higher will converge faster but more likely to get stuck in local optium (Default = .2).}

\item{prop_ignored}{The proportion of the population that are given a fixed fitness value, thus their importance is greatly reduce. The higher it is, the longer it takes to converge. Highers values makes the algorithm focus more on favorizing the good subsets of variables than penalizing the bad subsets (Default = .50).}

\item{search_criterion}{Criterion used to determine which variable subset is the best. If \code{search_criterion="AIC"}, uses the AIC, if \code{search_criterion="AICc"}, uses the AICc, if \code{search_criterion="BIC"}, uses the BIC, if \code{search_criterion="cv"}, uses the cross-validation error, if \cr \code{search_criterion="cv_AUC"}, uses the cross-validated AUC, if \code{search_criterion="cv_Huber"}, uses the Huber cross-validation error, if \code{search_criterion="cv_AUC"}, uses the L1-norm cross-validation error (Default = "AIC"). The Huber and L1-norm cross-validation errors are alternatives to the usual cross-validation L2-norm error (which the \eqn{R^2} is based on) that are more resistant to outliers, the lower the values the better.}

\item{n_cluster}{Number of parallel clusters, I recommend using the number of CPU cores (Default = 1).}

\item{eps}{Threshold for convergence (.01 for quick batch simulations, .0001 for accurate results). Note that using .001 rather than .01 (default) can more than double or triple the computing time of genetic_var_select.}

\item{maxiter}{Maximum number of iterations.}

\item{family}{Outcome distribution and link function (Default = gaussian).}

\item{ylim}{Optional vector containing the known min and max of the outcome variable. Even if your outcome is known to be in [a,b], if you assume a Gaussian distribution, predict() could return values outside this range. This parameter ensures that this never happens. This is not necessary with a distribution that already assumes the proper range (ex: [0,1] with binomial distribution).}

\item{seed}{Optional seed.}

\item{progress}{If TRUE, shows the progress done (Default=TRUE).}

\item{cv_iter}{Number of cross-validation iterations (Default = 5).}

\item{cv_folds}{Number of cross-validation folds (Default = 10). Using \code{cv_folds=NROW(data)} will lead to leave-one-out cross-validation.}

\item{folds}{Optional list of vectors containing the fold number for each observation. Bypass cv_iter and cv_folds. Setting your own folds could be important for certain data types like time series or longitudinal data.}

\item{Huber_p}{Parameter controlling the Huber cross-validation error (Default = 1.345).}

\item{classification}{Set to TRUE if you are doing classification and cross-validation (binary outcome).}
}
\value{
Returns nothing (everything is printed)
}
\description{
Use natural evolution strategy to find the best 2 subsets of variables for latent A x B interaction models.
}
\examples{
\dontrun{
## Example
 ## Looks at E x G, E x Z and G x Z
train = example_3way_3latent(250, 2, seed=777)
nes = var_select_interactions_2way(x=c("E","G","Z"),y="y",train$data)
}
}
